import java.io.*;

import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.mapreduce.lib.input.*;
import org.apache.hadoop.mapreduce.lib.output.*;


public class SpeedData {
public static class mapperclass extends Mapper<LongWritable,Text,Text,FloatWritable>
{
	public void map(LongWritable key,Text value,Context context)
	{
		Text vehiclenum=new Text();
		FloatWritable speed=new FloatWritable();
		try
		{
			String[] vehicles=value.toString().split(",");
			vehiclenum.set(vehicles[0]);
			speed.set(Integer.parseInt(vehicles[1]));
			context.write(vehiclenum,speed);
	}
		catch(Exception e)
		{
			System.out.println(e);
		}
}
}
public static class reduceclass extends Reducer<Text,FloatWritable,Text,FloatWritable>
{
public void reduce(Text key,Iterable<FloatWritable> value,Context context) throws IOException, InterruptedException
{
	float offencecount=0;
	float speed=0;
	float totalcount=0;
	for(FloatWritable v:value)
	{
		speed=v.get();
		if(speed > 65.0)
		{
			offencecount+=1;
		}
		totalcount+=1;
	}
context.write(new Text(key),new FloatWritable((offencecount/totalcount)*100));
}


}
public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "Volume Count");
    job.setJarByClass(SpeedData.class);
    job.setMapperClass(mapperclass.class);
    job.setReducerClass(reduceclass.class);
    job.setNumReduceTasks(1);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(FloatWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}

